{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f688f9ac-ebc1-475c-a278-5e7242b99802",
   "metadata": {},
   "source": [
    "# Modularizace základních operací"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9aabd-6fd0-4329-9239-0ec347de0db9",
   "metadata": {},
   "source": [
    "Úkolem cvičení je implementovat základní operace jako lineární funkce a sigmoid používané v neurosítích jako funkční bloky tak, aby je bylo možné skládat za sebe v libovolném pořadí. Návrh přebírá a kopíruje od knihovny PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb7b35-c756-4246-aff2-03ab3928ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375f030-c696-4b7d-a808-5ab895104166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import sys\n",
    "sys.path.append('..')  # import tests\n",
    "\n",
    "import torch\n",
    "\n",
    "import ans\n",
    "from tests import test_neural_library\n",
    "from tests.test_neural_library import randn_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27932d6-b1a2-4284-b7be-64a77ec4da7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fukcionální rozhraní: modul `ans.functional` a třída `Function`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393a8db-6a22-45ca-b997-367622b7d2d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ve cvičení [backpropagation.ipynb](backpropagation.ipynb) byly operace nad `Variable` definovány přepisem tzv. magických metod pro odpovídající operátory této třídy. Tento způsob je vhodný pro základní operace jako sčítání a násobení. Pro složitější operace jako konvoluce či rekurence však v Pythonu operátory, které bychom mohli definovat, neexistují. Navíc mohou mít tyto operace nepovinné parametry, které by nebylo jednoduché předat.\n",
    "\n",
    "Operace s proměnnými `Variable` ale můžeme provádět i jako volání funkcí. Např. násobení dvou proměnných `u` a `v` by namísto `w = u * v` mohlo vypadat jako `w = multiply(u, v)`. Jednou z výhod takového návrhu je rozšiřitelnost i pro složitější operace s dodatečnými parametry. Funkce `multiply` by mohla vypadat takto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a807c24-4bdf-4bd9-8a0f-19f3a9cd5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: ans.autograd.Variable, b: ans.autograd.Variable) -> ans.autograd.Variable:\n",
    "    def grad_fn(dout: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return dout * b.data, dout * a.data  \n",
    "    return ans.autograd.Variable(\n",
    "        a.data * b.data,\n",
    "        parents=(a, b),\n",
    "        grad_fn=grad_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b839b-32ae-4826-9236-c4b720c5430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ans.autograd.Variable(2.)\n",
    "v = ans.autograd.Variable(3.)\n",
    "w = multiply(u, v)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29eb318-2234-4132-a847-a17257c2bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad_fn(torch.tensor(1.))  # du, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ce126-59fc-4ca8-975c-9ba51d23e286",
   "metadata": {},
   "source": [
    "Analogicky bychom mohli definovat i sčítání."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69465ed5-b09b-439c-882b-79ded6fb0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: ans.autograd.Variable, b: ans.autograd.Variable) -> ans.autograd.Variable:\n",
    "    def grad_fn(dout: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return dout.clone(), dout.clone()\n",
    "    return ans.autograd.Variable(\n",
    "        a.data + b.data,\n",
    "        parents=(a, b),\n",
    "        grad_fn=grad_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515a835-ab07-4028-bfc6-b568d5fcfd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ans.autograd.Variable(2.)\n",
    "v = ans.autograd.Variable(3.)\n",
    "w = add(u, v)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295959d4-564b-4369-b195-36d8e20268f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad_fn(torch.tensor(2.))  # du, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ca001-a270-4417-8ffb-717d7a0729d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tzv. boilerplate kód, kdy se musíme neustále starat o \"vytahování\" atributu `data: torch.Tensor` z `Variable` a vytváření `Variable` z výstupu se správně nastavenými `parents` atp., lze zautomatizovat. Abychom toho dosáhli, nejprve každou operaci nad objekty `Variable` jako `add` či `multiply` principiálně rozdělíme na dopředný a zpětný průchod, tj. např. jako `multiply_forward` a `multiply_backward`, přičemž oba budou pracovat se vstupy typu `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88ff4f-3687-43b9-8c53-41be0d8d4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_forward(a: torch.Tensor, b: torch.Tensor) -> tuple[torch.Tensor, tuple]:\n",
    "    output = a * b\n",
    "    cache = a, b\n",
    "    return output, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31ba80-54db-406a-ab22-43524b15f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_backward(doutput: torch.Tensor, cache=()) -> tuple[torch.Tensor, ...]:\n",
    "    a, b = cache\n",
    "    return doutput * b, doutput * a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70ede8-c1e6-42ae-b759-ee9438e8af02",
   "metadata": {},
   "source": [
    "Poté bude existovat třetí metoda `apply`, která bude zajišťovat jejich správné volání s proměnnými `Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a6734-1d14-4826-ade6-37af8cdc54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_apply(*inputs: ans.autograd.Variable, **params) -> ans.autograd.Variable:\n",
    "    tensor_args = [i.data for i in inputs]\n",
    "    output_data, cache = multiply_forward(*tensor_args, **params)\n",
    "    return ans.autograd.Variable(\n",
    "        output_data,\n",
    "        parents=inputs,\n",
    "        grad_fn=functools.partial(multiply_backward, cache=cache)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af2d93-387a-4873-8b06-fe424e0552cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ans.autograd.Variable(2.)\n",
    "v = ans.autograd.Variable(3.)\n",
    "w = multiply_apply(u, v)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c108120-1619-4e11-928d-2fb320187e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.parents[0] is u, w.parents[1] is v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf305ce-b457-48f9-91df-355a9a5f3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad_fn(torch.tensor(1.))  # du, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2e5ef-a569-4abe-9ab9-511d81fd802a",
   "metadata": {},
   "source": [
    "Zatím jsme si příliš nepomohli, protože namísto jedné teď máme metody tři a ještě specializované výhradně na operaci násobení. \"Mustr\" ovšem zobecníme a zabalíme do obecné šablony v podobě základní třídy `Function` v modulu `ans.functional`. Všechny operace, které budeme definovat, budou odvozeny (dědit z) z této třídy, což mimo jiné znamená, že `apply` bude existovat pouze jedna.\n",
    "\n",
    "Základní třída `Function` má tři metody:\n",
    "\n",
    "``` python\n",
    "@staticmethod\n",
    "def forward(*inputs: torch.Tensor, **params: Any) -> tuple[torch.Tensor, tuple]\n",
    "```\n",
    "Funkce deklaruje obecnou podobu dopředného průchodu jakékoliv diferencovatelné operace. Převezme\n",
    "- vstupy `inputs: torch.Tensor`\n",
    "- příp. nepovinné parametry `params`\n",
    "\n",
    "a bude vždy vracet dvojici `(output, cache)`, kde\n",
    "- `output: torch.Tensor` je výsledek operace\n",
    "- `cache: tuple[Any, ...]` je n-tice, ve které jsou uloženy mezivýsledky potřebné pro výpočet zpětného průchodu\n",
    "\n",
    "``` python\n",
    "@staticmethod\n",
    "def backward(doutput: torch.Tensor, cache=()) -> tuple[torch.Tensor, ...]\n",
    "```\n",
    "Deklaruje rozhraní zpětného průchodu dané operace. Převezme\n",
    "- příchozí gradient `doutput: torch.Tensor`\n",
    "- mezipaměť z dopředného průchodu `cache`\n",
    "\n",
    "a vrátí\n",
    "- propagovaný gradient `dinput0` na vstup `inputs[0]`,\n",
    "- propagovaný gradient `dinput1` na vstup `inputs[1]`,\n",
    "\n",
    "``` python\n",
    "@classmethod\n",
    "def apply(cls, *inputs: Union[torch.Tensor, Variable], **params: Any) -> Variable\n",
    "```\n",
    "Je hlavní metoda, kterou budeme volat pro provedení dané operace. Tuto metodu odvozené třídy nebudou přepisovat a bude tak společná pro všechny. Zajišťuje:\n",
    "- automatické \"obalení\" výstupu typu `torch.Tensor` z dopředného průchodu `forward()` třídou `Variable`\n",
    "- nastavení rodičů (`parents: tuple[Variable]`) tak, aby ukazovaly na ty vstupy `inputs`, které jsou typu `Variable` a zajímá nás jejich gradient\n",
    "- nastavení `grad_fn` na zpětný průchod dané operace, tj. `backward` stejné třídy s předpřipravenou `cache`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8f9c5-7b1d-4019-947c-cf566f92cdee",
   "metadata": {},
   "source": [
    "## Násobení: třída `Multiply`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29500fb9-f1f2-4b2f-b9b2-b9affe7424cc",
   "metadata": {},
   "source": [
    "Jako ukázku odvozování z třídy `Function` si můžeme implementovat třídu pro operaci násobení.\n",
    "\n",
    "**Dopředný průchod**\n",
    "\n",
    "Převezme libovolný počet tensorů $x_1, \\ldots, x_M$ a prvkově je pronásobí mezi sebou jako\n",
    "$$y = x_1 \\odot x_2 \\odot \\ldots \\odot x_M = \\prod_{m=1}^{M}x_m$$\n",
    "(tensory by měly být stejných rozměrů).\n",
    "\n",
    "**Zpětný průchod**\n",
    "\n",
    "Lokální gradient, tj. parciální derivace $dy/dx_m$, $m=1,\\ldots,M$ je na libovolný vstup $x_m$\n",
    "$$\\frac{dy}{dx_m} = \\prod_{i=1, i \\ne m}^{M}x_m = \\frac{y}{x_m}$$\n",
    "\n",
    "Zpětný průchod řetízkovým pravidlem při nějakém příchozím gradientu $\\overline{y}$ bude\n",
    "$$\\overline{x_m} = \\overline{y} \\odot \\frac{dy}{dx_m} = \\overline{y} \\odot \\frac{y}{x_m}$$\n",
    "\n",
    "To znamená, že v dopředném průchodu potřebujeme do `cache` uložit všechny vstupy $x_1, \\ldots, x_M$ a výsledek pronásobení $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509ae52-5abb-4636-9001-305028db7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(ans.functional.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(*inputs: torch.Tensor) -> tuple[torch.Tensor, tuple]:\n",
    "        outputs = inputs[0].clone()\n",
    "        for inp in inputs[1:]:\n",
    "            outputs *= inp\n",
    "        cache = inputs, outputs\n",
    "        return outputs, cache\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(doutputs: torch.Tensor, cache=()) -> tuple[torch.Tensor, ...]:\n",
    "        inputs, outputs = cache\n",
    "        return tuple(doutputs * outputs / inp for inp in inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a7dde-3b56-446f-823d-ff3818e4d31f",
   "metadata": {},
   "source": [
    "Funkci `Multiply` chceme aplikovat na proměnné `Variable`, zatímco obě metody `Multiply.forward()` i `Multiply.backward()` přebírají `torch.Tensor`. Volat funkci proto budeme skrze metodu `apply()`, která provede automatickou konverzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f00f7-50f9-4ea1-b756-0ae76f8a8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = ans.autograd.Variable(2.)\n",
    "v = ans.autograd.Variable(3.)\n",
    "w = Multiply.apply(u, v)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98e25e-8562-48c3-8d39-8c90d799b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.parents[0] is u, w.parents[1] is v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a18912-17e5-436a-92c9-be754676b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da1d47-1ab4-42f3-a1ec-84509a70670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad_fn(1.)  # du, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b787b-3ddc-4e08-a0b2-0e8a33072e4b",
   "metadata": {},
   "source": [
    "## Kontrola gradientů: `ans.autograd.gradcheck()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9795f-039a-4c0e-b546-50b3bf01fc57",
   "metadata": {},
   "source": [
    "Abychom si byli jistější, že jsme násobení implementovali správně, můžeme implementaci numericky zkontrolovat. Pokud některý ze vstupů nepatrně změníme, měl by se odpovídajícím způsobem změnit i výsledek. Analyticky odvozený gradient přitom vyjadřuje, jak velká tato změna bude v poměru ke změně na vstupu, pokud ta se limitně blíží nule. Pokud tedy vstup změníme o dostatečně malou hodnotu, musí se výsledek rovnat gradientu vrácenému ze zpětného průchodu. Pokud se oba výsledky nerovnají až na nějakou numerickou toleranci, chyba musí být na straně odvození či implementace zpětného průchodu. Numerický gradient se mýlit nemůže, protože měří skutečné změny na výstupu po spuštění dopředného průchodu se změněným vstupem. Celou takovouto kontrolu provádí metoda `ans.autograd.gradcheck()` a vlastně pouze ověřuje, zda si dopředný a zpětný průchod operace odpovídají. Může se tedy i stát, že `gradcheck` selže kvůli implementační chybě ve `forward` a přitom `backward` bude správně vůči analyticky odovzeným vztahům."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c00262-7bfd-4e0b-885b-15467393ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.autograd.gradcheck?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bfbb4-fbfa-4449-beef-a4775bf0ec67",
   "metadata": {},
   "source": [
    "Funkce `ans.autograd.gradcheck` převezme\n",
    "- nějakou funkci `func`, do které vstupují i ze které vystupují `ans.autograd.Variable`,\n",
    "- vstupy `inputs` typu `torch.Tensor` nebo `ans.autograd.Variable`,\n",
    "- případné nepovinné parametry `params`,\n",
    "- velikost změny na vstupu `eps` (pokud `None`, pak se spočítá automaticky jako 0.01 * standardní odchylka vstupu)\n",
    "- relativní povolenou toleranci `rtol`,\n",
    "- absolutní povolenou toleranci `atol`,\n",
    "- zda se mají či nemají vypisovat odchylky mezi analytickými a numerickými gradienty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d943034a-fb0a-4678-8688-d7882aca96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.autograd.gradcheck(Multiply.apply, (randn_var(3, 4), randn_var(3, 4), randn_var(3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812db4e-c7a2-4133-b4ba-c13259e0e76f",
   "metadata": {},
   "source": [
    "Pro ověření si lze zkusit záměrně vytvořit chybu ve zpětném průchodu. Změňte zpětný průchod `Multiply.backward()` např. na\n",
    "``` python\n",
    "return tuple(doutputs * outputs / inpupts[0] for inp in inputs)\n",
    "```\n",
    "a spusťte `gradcheck` znovu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e754247-b411-4b41-8ef1-35eb5662796c",
   "metadata": {},
   "source": [
    "## Lineární (afinní) funkce $z = x \\cdot W + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1809e7-2cbf-48e9-97e0-c72dfb7bf5eb",
   "metadata": {},
   "source": [
    "Jedním ze základních stavebních kamenů neurosítí je afinní operace, většinou nesprávně označovaná jako lineární (platí pouze bez biasu):\n",
    "\n",
    "**Dopředný průchod**\n",
    "\n",
    "$$\\boldsymbol{z}_n = \\boldsymbol{x}_n \\cdot \\boldsymbol{W} + \\boldsymbol{b}$$\n",
    "\n",
    "- $\\boldsymbol{x}_n = [x_{n1}, \\ldots, x_{nD}]$ je *řádkový* vektor s rozměrem $D$\n",
    "- $\\boldsymbol{W} = [w_{dc}]$ je váhová matice tvaru $D \\times C$, tj. s $D$ řádky a $C$ sloupci\n",
    "- $\\boldsymbol{b} = [b_1, \\ldots, b_C]$ je řádkový vektor biasů s rozměrem $C$\n",
    "- $\\boldsymbol{z}_n = [z_{n1}, \\ldots, z_{nC}]$ je řádkový vektor výstupních lineárních skóre s rozměrem $C$\n",
    "\n",
    "**Zpětný průchod**\n",
    "\n",
    "$$\\overline{\\boldsymbol{x}_n} = \\overline{\\boldsymbol{z}_n} \\cdot \\boldsymbol{W}^\\top$$\n",
    "$$\\overline{\\boldsymbol{W}} = \\sum_{n=1}^N{\\boldsymbol{x}_n^\\top \\cdot \\overline{\\boldsymbol{z}_n}}$$\n",
    "$$\\overline{\\boldsymbol{b}} = \\sum_{n=1}^N{\\overline{\\boldsymbol{z}_n}}$$\n",
    "\n",
    "- $\\overline{x}_n, \\overline{W}, \\overline{b}$ jsou propagované \"odchozí\" gradienty na $\\boldsymbol{x}_n$, $W$, resp. $\\boldsymbol{b}$; každý gradient $\\overline{\\boldsymbol{v}}$ má vždy stejný rozměr jako odpovídající vstup $\\boldsymbol{v}\\in\\{\\boldsymbol{x}, \\boldsymbol{W}, \\boldsymbol{b}, \\boldsymbol{z}\\}$\n",
    "- $\\overline{\\boldsymbol{z}_n} = [\\overline{z_1}, \\ldots, \\overline{z_C}]$ je \"příchozí\" gradient na výstup $\\boldsymbol{z}_n$\n",
    "- $N$ je počet vzorků v dávce (batch size)\n",
    "\n",
    "**Dávkové zpracování**\n",
    "\n",
    "Vrstvu naimplementujeme tak, aby vstupy byly\n",
    "- **dávka vektorů** $\\boldsymbol{X} = [\\boldsymbol{x}_1; \\ldots; \\boldsymbol{x}_N]$ jako matice s rozměry $N \\times D$, tzn. jednotlivé vektory $\\boldsymbol{x}_n$ jdou po řádcích a $N$ značí velikost dávky (batch size)\n",
    "- váhová matice $\\boldsymbol{W}$\n",
    "- bias vektor $\\boldsymbol{b}$\n",
    "\n",
    "Výstup bude\n",
    "- matice $\\boldsymbol{Z} = [\\boldsymbol{z}_1; \\ldots; \\boldsymbol{z}_N]$ s rozměry $N \\times C$\n",
    "\n",
    "**Všechny další funkce budou takto zamýšleny rovněž, tj. s první dimenzí vstupního tensoru odpovídající jednotlivým vzorkům (např. obrázkům či vektorům) v dávce.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b01b7-b6bd-4bf3-acc0-68e0951faf68",
   "metadata": {},
   "source": [
    "### TODO: implementuje třídu `ans.functional.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f765c2-3aab-4963-9a62-b9bb0d353e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.autograd.gradcheck(\n",
    "    ans.functional.Linear.apply,\n",
    "    (randn_var(2, 4, name='input'), randn_var(4, 3, name='weight'), randn_var(3, name='bias'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ea99d-682e-409c-be4e-f171de188cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neural_library.TestLinearFunction.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61099930-1ffe-4997-9962-2c056278fba6",
   "metadata": {},
   "source": [
    "## Sigmoid aktivace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da523a-81cf-4e36-b4da-2a2d1ce0dcac",
   "metadata": {},
   "source": [
    "Mezi lineární vrstvy budeme pro začátek vkládat sigmoid nelinearitu a až později vyměníme za ReLU.\n",
    "\n",
    "**Dopředný průchod**\n",
    "\n",
    "$$z = \\frac{1}{1 + e^{-x}}$$\n",
    "- $x$ je reálné číslo (skalár)\n",
    "- $z$ je reálně číslo (skalár)\n",
    "\n",
    "**Zpětný průchod**\n",
    "\n",
    "$$\\overline{x} = \\overline{z} \\cdot z \\cdot (1 - z)$$\n",
    "- $\\overline{z}$ je příchozí gradient na $z$\n",
    "\n",
    "**Dávkové zpracování**\n",
    "\n",
    "Operace sigmoid aplikujeme na všechny prvky vstupu nezávisle na sobě."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af92e8e-7c5b-481e-adb8-fc2ff844175f",
   "metadata": {},
   "source": [
    "### TODO: implementujte třídu `ans.functional.Sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bb8ed-d01e-4f3b-a25d-78d476594316",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.autograd.gradcheck(\n",
    "    ans.functional.Sigmoid.apply,\n",
    "    (randn_var(2, 4, name='input', dtype=torch.float32),)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932dc1d-4bea-49c1-9000-3626c50b908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neural_library.TestSigmoidFunction.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ea49c-0180-4b10-b728-935c5d56d00a",
   "metadata": {},
   "source": [
    "## Křížová entropie pro klasifikaci softmaxem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4467cf-0982-4214-af01-ddb69506c825",
   "metadata": {},
   "source": [
    "Podobně jako v prvním cvičení [linear-classification.ipynb](linear-classification.ipynb) si křížovou entropii zadefinujeme tak, aby vstupem byla skóre neznormalizovaná softmaxem, tj. např. výstup z poslední lineární vrstvy.\n",
    "\n",
    "**Dopředný průchod**\n",
    "\n",
    "$$\n",
    "l_n = \\log\\frac{\\exp{z_{ny_n}}}{\\sum_{i=1}^{C}\\exp{z_{ni}}}\n",
    "$$\n",
    "- $\\boldsymbol{z}_n = [z_{n1}, \\ldots, z_{nC}]$ je řádkový vektor (rozměr $C$), typicky \"před-softmax\" skóre z poslední vrstvy predikované klasifikátorem pro jeden aktuální vstup\n",
    "- $y_n \\in \\{1, \\ldots, C\\}$ je číslo (skalár) označující správnou třídu pro jeden aktuální vstup\n",
    "- $l_n$ je výsledná skalární hodnota vyjadřující, jak dobře pravděpodobnosti predikované klasifikátorem pro jednotlivé třídy odpovídají cílovému rozdělení pro jeden aktuální vstup\n",
    "\n",
    "**Zpětný průchod**\n",
    "\n",
    "$$\n",
    "\\overline{z_{nc}} = \\frac{\\exp{z_{nc}}}{\\sum_{i=1}^{C}\\exp{z_{ni}}} - \\mathbb{1}(c = y_n)\n",
    "$$\n",
    "<!-- - $\\overline{\\boldsymbol{z}} = [\\overline{z_1}, \\ldots, \\overline{z_C}]$ je příchozí gradient  -->\n",
    "- $\\mathbb{1}(\\cdot)$ je rovno 1, pokud je splněna podmínka $\\cdot$, jinak 0\n",
    "\n",
    "**Dávkové zpracování**\n",
    "\n",
    "Vstupy budou\n",
    "- dávka predikovaných skóre $\\boldsymbol{Z} = [\\boldsymbol{z}_1; \\ldots; \\boldsymbol{z}_N]$ jako matice s rozměry $N \\times C$, tzn. jednotlivé vektory $\\boldsymbol{z}_n$ jdou po řádcích a $N$ značí velikost dávky (batch size)\n",
    "- dávka \"targetů\" $\\boldsymbol{y} = [y_1, \\ldots, y_N]$ jako vektor s rozměrem $N$\n",
    "\n",
    "Výstupy budou\n",
    "- celkový loss $l = \\frac{1}{N}\\sum_{n=1}^Nl_n$ jako průměr dílčích lossů za jednotlivé vektory predikcí $\\boldsymbol{z}_n$\n",
    "\n",
    "**Numerická stabilita**\n",
    "\n",
    "Během kontroly gradientů, kdy se pracuje s datovým typem `float64`, se nejspíše žádné problémy s numerickou (ne)stabilitou neprojeví. Pokud ovšem budeme funkci `SoftmaxCrossEntropy` používat s 32bitovými reálnými čísly `float32`, může vlivem umocňování funkcí `exp` docházet k přetékání numerické přenosti pro $z_c \\gg 1$. Odolnost funkce vůči příliš velkým vstupům lze vylepšit odečtením maxima z každého vstupního vektoru\n",
    "$$\\boldsymbol{z}_n := \\boldsymbol{z}_n - \\max(\\boldsymbol{z}_n)$$\n",
    "Takže výpočet bude ve skutečnosti\n",
    "$$\n",
    "l_n = \\log\\frac{\\exp{(z_{ny} - \\max(\\boldsymbol{z}_n))}}{\\sum_{i=1}^{C}\\exp{(z_{ni} - \\max(\\boldsymbol{z}_n))}}\n",
    "$$\n",
    "před výpočtem $l$ v dopředném průchodu. Výpočet $l$ to neovlivní, protože pro libovolná reálná čísla $z$, $a$ platí\n",
    "$$\\exp{(z - a)} = \\exp{z}\\cdot\\exp{(-a)}$$\n",
    "a člen $\\exp{(-a)}$ lze vytknout z čitatele i jmenovatele a tím pádem se normalizace odečtením maxima vykrátí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350b044-20c5-42e1-8d65-2fce5fe2fe2c",
   "metadata": {},
   "source": [
    "### TODO: implementujte třídu `ans.functional.SoftmaxCrossEntropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537495e1-574d-46fb-ad15-b8b1c3184e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.autograd.gradcheck(\n",
    "    ans.functional.SoftmaxCrossEntropy.apply,\n",
    "    (randn_var(10, 4, name='scores'), torch.randint(4, (10,)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860333c4-2e9e-4d41-be96-f6c8d5b84c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neural_library.TestSoftmaxCrossEntropyFunction.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864f81b-d460-495a-b954-1e4598bd3f72",
   "metadata": {},
   "source": [
    "## Dvouvrstvý perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc56a40-c823-4691-99cc-22b0fd8c2f2a",
   "metadata": {},
   "source": [
    "Základní bloky `Linear` a `Sigmoid` jsme navrhli jako funkce, takže je můžeme libovolně volat a skládat za sebe. Např. vícevrstvý bychom nyní mohli implementovat následovně. Nejprve inicializujeme \"toy data\" s deseti vstupními vektory, které budeme klasifikovat do 3 tříd. Jelikož gradienty na data nás nezajímají, budou to \"neobalené\" `torch.Tensor`y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69470be2-f722-4e8a-9f9b-0a611d55d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(10, 5)  # toy data\n",
    "y = torch.randint(3, (10,))  # toy targets with 3 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e98150-266b-485c-be17-e4ddd1245a44",
   "metadata": {},
   "source": [
    "Poté incializujeme parametry sítě:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd7f00-8046-43dc-8d9d-85c40046bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = ans.autograd.Variable(0.001 * torch.randn(5, 4))  # weight matrix for layer 1\n",
    "b1 = ans.autograd.Variable(torch.zeros(4))  # bias vector for layer 1\n",
    "W2 = ans.autograd.Variable(0.001 * torch.randn(4, 3))  # weight matrix for layer 2\n",
    "b2 = ans.autograd.Variable(torch.zeros(3))  # bias vector for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3722ac0-2bd3-4608-829d-8623ddc47240",
   "metadata": {},
   "source": [
    "Provedeme dopředný průchod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe353a7-9801-4696-ae49-81ee92a35885",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = ans.functional.Linear.apply(X, W1, b1)\n",
    "h1 = ans.functional.Sigmoid.apply(z1)\n",
    "z2 = ans.functional.Linear.apply(h1, W2, b2)\n",
    "z2  # predicted scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876254b9-803d-41c0-aac8-ede6b731109e",
   "metadata": {},
   "source": [
    "Spočítáme loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab81446-d5b5-4893-9db9-ed30988180c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ans.functional.SoftmaxCrossEntropy.apply(z2, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d8b2f-0890-4d16-9185-7297413cccfc",
   "metadata": {},
   "source": [
    "V grafu neuvidíme všechny uzly, ale pouze takové, které jsou typu `Variable`. Uzly typu `torch.Tensor` se totiž nezapisují do atributu `parents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec3d17-84b5-49ed-b5c3-fe3116a87977",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.to_graphviz()  # re-run this a few times if variables have names like '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6333d-c818-4ed1-90b2-226378d7b18c",
   "metadata": {},
   "source": [
    "Provedeme zpětnou propagaci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbdfbc-1fde-4962-9ab1-1ff7b910660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5102f6-68da-4a84-a0d9-6be4ca10d729",
   "metadata": {},
   "source": [
    "Konečně trochu pocítíme výhodu automatického výpočtu gradientů, tzv. autogradu. Všechny uzly v grafu typu `Variable` teď mají vyplněny atribut `grad`, který obsahuje gradient $\\partial l/\\partial u$, kde $l$ je celkový loss a $u$ je nějaký uzel. Jelikož vstup jsme nechali jako `torch.Tensor`, gradienty jsme obdrželi pouze na parametry sítě `W1, b1, W2, b2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c48a7-b326-411b-afdb-ffeff284ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99936173-d09d-42a1-bbc9-8980bad59b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0ba62-8563-4c3d-9328-9749e0b30a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abf8fa-5fca-46fc-95e4-835d5a4adf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b164ab-df04-49b0-8b2f-8c554caa7f81",
   "metadata": {},
   "source": [
    "Pokud bychom nyní chtěli síť trénovat, updatovali bychom gradienty např. standardním SGD pravidlem jako např. `W1.data -= learning_rate * W1.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325cd17d-95fc-47b9-8b75-2c27388be22b",
   "metadata": {},
   "source": [
    "# Objektové rozhraní: `ans.modules` a třída `Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde627e3-8f91-49e5-9031-653540a9e11e",
   "metadata": {},
   "source": [
    "Systém ovšem stále ještě není optimálně konfigurovatelný.\n",
    "1. Musíme manuálně inicializovat parametry. Lepší by bylo, kdybychom např. u funkce `Linear` mohli jednoduše zadat požadované rozměry vstupu a výstupu a vše ostatní by bylo schované v její implementaci.\n",
    "2. Musíme rovněž specifikovat dopředný průchod. Mohli bychom vše např. navrhnout tak, že zadáme pouze posloupnost požadovaných operací, která se má se vstupem v daném pořadí provést.\n",
    "\n",
    "Základní stavební bloky si proto navrhneme tak, abychom je mohli používat jako objekty. Např. lineární vrstvu zadefinujeme jako třídu `Linear` a budeme používat takto:\n",
    "``` python\n",
    "x = torch.randn(10, 5)  # data (num_inputs, input_size)\n",
    "linear = ans.modules.Linear(5, 4)  # (input_size, output_size)\n",
    "z = linear(x)  # z = xW + b\n",
    "```\n",
    "\n",
    "Všechny takto navržené vrstvy umístíme do modulu `ans.modules` a budou dědit od základní třídy `Module`, která implementuje funkcionalitu společnou pro všechny vrstvy - např. navrácení seznamu parametrů či přepis magické metody `__call__` apod. - a zamezuje tak zbytečné duplicitě kódu. Každá vrstva odvozená od `Module` musí přepsat:\n",
    "1. `forward`, kde definuje dopředný průchod,\n",
    "2. nepovinně `__init__`, kde vyřeší příp. inicializaci parametrů apod.\n",
    "\n",
    "Nebudeme přitom samozřejmě reimplementovat lineární a sigmoid funkce znovu, ale z vrstev odvozených z `Module` budeme volat funkce z modulu `ans.functional`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc663a-1174-43ca-a803-2ade9c603d27",
   "metadata": {},
   "source": [
    "## Vrstva `Linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc7562-d0d5-4630-b42a-fed470b30fc4",
   "metadata": {},
   "source": [
    "Lineární funkce jako vrstva by měla:\n",
    "- uchovávat v sobě a v metodě `__init__` automaticky vhodně inicializovat parametry $\\boldsymbol{W}$ a $\\boldsymbol{b}$, tedy váhy a bias\n",
    "- v dopředném průchodu `forward` vypočítat výstup jako $\\boldsymbol{z}_n = \\boldsymbol{x}_n\\cdot\\boldsymbol{W} + \\boldsymbol{b}$\n",
    "\n",
    "Inicializovat váhy budeme jednou z variant metody Xavier, kde prvky váhové matice $W = [w_{dc}]$ s rozměry $D \\times C$ pocházejí z rovnoměrného náhodného rozdělení\n",
    "$$\n",
    "w_{dc} \\sim \\mathcal{U}\\left(\\frac{-1}{\\sqrt D}, \\frac{+1}{\\sqrt D}\\right)\n",
    "$$\n",
    "\n",
    "Bias inicializujeme na nuly.\n",
    "\n",
    "Dopředný průchod by měl volat `ans.functional.Linear.apply`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884ff10-874a-4e8a-a824-4cb27053923f",
   "metadata": {},
   "source": [
    "### TODO: implementujte vrstvu `Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaeee9a-9525-4bd5-87b9-9f745061f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neural_library.TestLinearModule.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a4f47-8f07-4acd-8af6-da88affb8d17",
   "metadata": {},
   "source": [
    "## Vrstva `Sigmoid`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b578e6-783b-44ba-954e-3ca4723261cf",
   "metadata": {},
   "source": [
    "Funkce Sigmoid jako vrstva bude jednodušší. Nemá žádné parametry, a tak nám bude stačit přepsat pouze `forward` tak, aby volal `ans.functional.Sigmoid.apply`. I bez parametrů bude objektově navržený sigmoid výhodný, protože ho půjde jednoduše přidat jako vrstvu do obecné dopředné vícerstvé sítě (multilayer perceptronu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f90cf-23d4-4a48-ae14-a7ea01efb2ad",
   "metadata": {},
   "source": [
    "### TODO: implementujte vrstvu `Sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcc728-ea34-4e38-86d1-1a22c9362737",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neural_library.TestSigmoidModule.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a4cbb-1309-4c4d-8ba1-7c70d73ed79a",
   "metadata": {},
   "source": [
    "## Vrstva `SoftmaxCrossEntropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2675468-fb41-4122-8b9c-baecb58dd465",
   "metadata": {},
   "source": [
    "Implementace `SoftmaxCorssEntropy` bude jednoduchá podobně jako vrstva `Sigmoid`, protože opět nemá žádné parametry. Dopředný průchod `forward` tedy stačí pouze definovat, aby volal `ans.functional.softax_corss_entropy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35e216-5e1a-4fa8-8503-4f998f045067",
   "metadata": {},
   "source": [
    "### TODO: implementujte vrstvu `SoftmaxCrossEntropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3aa8c-ec30-4601-8258-c7900d80d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neural_library.TestSoftmaxCrossEntropyModule.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f90c4-a617-4ac9-85c2-6c1f6ae27f63",
   "metadata": {},
   "source": [
    "# Konfigurovatelný vícevrstvý perceptron pomocí třídy `ans.modules.Sequential`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e2bd0-f472-405a-9f76-72087643a330",
   "metadata": {},
   "source": [
    "Konečně máme definovány základní funkční bloky, které nyní můžeme libovolně skládat za sebe a snadno parametrizovat (předávat hyperparametry). Obecný vícevrstvý perceptron implementuje třída `ans.modules.Sequential`. Při vytváření převezme seznam vrstev v pořadí, v jakém by se měly postupně aplikovat na vstup. Funkcionalita kopíruje třídu [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) knihovny PyTorch. Viz příklad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558da33-c8e7-4181-a3d1-6bda796ef8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ans.modules.Sequential(\n",
    "    ans.modules.Linear(4, 4),\n",
    "    ans.modules.Sigmoid(),\n",
    "    ans.modules.Linear(4, 3),\n",
    "    ans.modules.Sigmoid(),\n",
    "    ans.modules.Linear(3, 2)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f161d89a-527e-47ca-9165-bbbfd265764b",
   "metadata": {},
   "source": [
    "Každá třída odvozená od `ans.modules.Module` dědí metodu `named_parameters()`, která projde danou instanci a vrátí seznam všech jejích atributů typu `Variable`, přičemž jim zároveň přiřadí jméno a to i rekurzivně pro vnořené objekty. Metodu později využijeme např. pro optimalizaci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f027d5-67f0-4d1c-a162-2811afebbca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6af56-255e-4e44-8d0c-8f22fda4bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ans.autograd.Variable(torch.randn(5, 4).float())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd3966-e278-4160-80f3-5f12aa4b71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.autograd.gradcheck(model, (x,))  # calls backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10693eb5-7e51-4856-9ca2-eab679466c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()  # cleanup after backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e76480-1ea5-44da-a2e9-6e49dd4e04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4fbbb-b3ca-478a-a65c-e018f23ab887",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randint(2, (5,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c7827-4828-4a7a-b4eb-45bbcfd490ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ans.functional.SoftmaxCrossEntropy.apply(z, y)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b444d01-ca54-4dd3-bd8e-ec438fbcc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.to_graphviz(show_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f65e9-c29e-4a11-b6a1-834d56403287",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, par in model.named_parameters():\n",
    "    print(f\"{name}: {par.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca31a2-79fd-4c6b-98ab-2d813a75db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753081f-031c-4b45-8b3b-4ed2b7f9b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, par in model.named_parameters():\n",
    "    print(f\"{name}: {par.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a85ea9-92ad-4c03-a49b-7a20fb639508",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, par in model.named_parameters():\n",
    "    par.data -= 1e-3 * par.grad  # SGD update with learning rate of 0.001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ans22",
   "language": "python",
   "name": "ans22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
